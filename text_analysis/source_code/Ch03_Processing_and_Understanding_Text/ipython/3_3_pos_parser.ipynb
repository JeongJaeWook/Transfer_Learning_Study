{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# parser 큰그림\n",
    "\n",
    "출처 : https://www.nltk.org/book/ch07.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"architecture.png\" width=\"700\"><BR>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 오늘 다룰 내용 POS\n",
    "\n",
    "pos(part of speech tagging) 문법 정도의 의미로 pos(Point of sales아님 주의)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"pos.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommended POS Taggers"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 다음 터미널 명령어를 사용해 2점대의 교재 github 파일을 3점대 파이썬 버전으로 일괄 업데이트\n",
    "# 2to3 my_file.py              # shows output only on terminal\n",
    "# 2to3 -w my_file.py           # overwrites the file with python-3 code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon Aug 15 17:08:05 2016\n",
    "\n",
    "@author: DIP\n",
    "\"\"\"\n",
    "\n",
    "sentence = 'The brown fox is quick and he is jumping over the lazy dog'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jupyter notebook 에 pattern이 설치되어 있지 않은 경우 인스톨\n",
    "# !pip install pattern\n",
    "#안되면 !conda install pip \n",
    "#!pip install Pattern3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### universal 파서"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DET'), ('brown', 'ADJ'), ('fox', 'NOUN'), ('is', 'VERB'), ('quick', 'ADJ'), ('and', 'CONJ'), ('he', 'PRON'), ('is', 'VERB'), ('jumping', 'VERB'), ('over', 'ADP'), ('the', 'DET'), ('lazy', 'ADJ'), ('dog', 'NOUN')]\n",
      "[('The', 'DT'), ('brown', 'JJ'), ('fox', 'NN'), ('is', 'VBZ'), ('quick', 'JJ'), ('and', 'CC'), ('he', 'PRP'), ('is', 'VBZ'), ('jumping', 'VBG'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "# recommended tagger based on PTB\n",
    "import nltk\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "tagged_sent = nltk.pos_tag(tokens, tagset='universal')\n",
    "print(tagged_sent)\n",
    "\n",
    "from pattern.en import tag\n",
    "tagged_sent = tag(sentence)\n",
    "print(tagged_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"PoSTags.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 참고. : https://universaldependencies.org/u/pos/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 자신만의 파서를 만들어 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Mr.', 'NNP'), ('Vinken', 'NNP'), ('is', 'VBZ'), ('chairman', 'NN'), ('of', 'IN'), ('Elsevier', 'NNP'), ('N.V.', 'NNP'), (',', ','), ('the', 'DT'), ('Dutch', 'NNP'), ('publishing', 'VBG'), ('group', 'NN'), ('.', '.')]\n",
      "[('Carnival', 'NNP'), ('Cruise', 'NNP'), ('Lines', 'NNP'), ('Inc.', 'NNP'), ('said', 'VBD'), ('0', '-NONE-'), ('potential', 'JJ'), ('problems', 'NNS'), ('with', 'IN'), ('the', 'DT'), ('construction', 'NN'), ('of', 'IN'), ('two', 'CD'), ('big', 'JJ'), ('cruise', 'NN'), ('ships', 'NNS'), ('from', 'IN'), ('Finland', 'NNP'), ('have', 'VBP'), ('been', 'VBN'), ('averted', 'VBN'), ('*-1', '-NONE-'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# preparing the data\n",
    "from nltk.corpus import treebank\n",
    "data = treebank.tagged_sents()\n",
    "train_data = data[:3500]\n",
    "test_data = data[3500:]\n",
    "print(train_data[1])\n",
    "print(test_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1454158195372253\n",
      "[('The', 'NN'), ('brown', 'NN'), ('fox', 'NN'), ('is', 'NN'), ('quick', 'NN'), ('and', 'NN'), ('he', 'NN'), ('is', 'NN'), ('jumping', 'NN'), ('over', 'NN'), ('the', 'NN'), ('lazy', 'NN'), ('dog', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "# default tagger\n",
    "from nltk.tag import DefaultTagger\n",
    "dt = DefaultTagger('NN')\n",
    "\n",
    "print(dt.evaluate(test_data))\n",
    "\n",
    "print(dt.tag(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24039113176493368\n",
      "[('The', 'NN'), ('brown', 'NN'), ('fox', 'NN'), ('is', 'NNS'), ('quick', 'NN'), ('and', 'NN'), ('he', 'NN'), ('is', 'NNS'), ('jumping', 'VBG'), ('over', 'NN'), ('the', 'NN'), ('lazy', 'NN'), ('dog', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "# regex tagger\n",
    "from nltk.tag import RegexpTagger\n",
    "# define regex tag patterns\n",
    "patterns = [\n",
    "        (r'.*ing$', 'VBG'),               # gerunds\n",
    "        (r'.*ed$', 'VBD'),                # simple past\n",
    "        (r'.*es$', 'VBZ'),                # 3rd singular present\n",
    "        (r'.*ould$', 'MD'),               # modals\n",
    "        (r'.*\\'s$', 'NN$'),               # possessive nouns\n",
    "        (r'.*s$', 'NNS'),                 # plural nouns\n",
    "        (r'^-?[0-9]+(.[0-9]+)?$', 'CD'),  # cardinal numbers\n",
    "        (r'.*', 'NN')                     # nouns (default) ... \n",
    "]\n",
    "rt = RegexpTagger(patterns)\n",
    "\n",
    "print(rt.evaluate(test_data))\n",
    "print(rt.tag(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8607803272340013\n",
      "[('The', 'DT'), ('brown', None), ('fox', None), ('is', 'VBZ'), ('quick', 'JJ'), ('and', 'CC'), ('he', 'PRP'), ('is', 'VBZ'), ('jumping', 'VBG'), ('over', 'IN'), ('the', 'DT'), ('lazy', None), ('dog', None)]\n",
      "0.13466937748087907\n",
      "[('The', 'DT'), ('brown', None), ('fox', None), ('is', None), ('quick', None), ('and', None), ('he', None), ('is', None), ('jumping', None), ('over', None), ('the', None), ('lazy', None), ('dog', None)]\n",
      "0.08064672281924679\n",
      "[('The', 'DT'), ('brown', None), ('fox', None), ('is', None), ('quick', None), ('and', None), ('he', None), ('is', None), ('jumping', None), ('over', None), ('the', None), ('lazy', None), ('dog', None)]\n"
     ]
    }
   ],
   "source": [
    "## N gram taggers\n",
    "from nltk.tag import UnigramTagger\n",
    "from nltk.tag import BigramTagger\n",
    "from nltk.tag import TrigramTagger\n",
    "\n",
    "ut = UnigramTagger(train_data)\n",
    "bt = BigramTagger(train_data)\n",
    "tt = TrigramTagger(train_data)\n",
    "\n",
    "print(ut.evaluate(test_data))\n",
    "print(ut.tag(tokens))\n",
    "\n",
    "print(bt.evaluate(test_data))\n",
    "print(bt.tag(tokens))\n",
    "\n",
    "print(tt.evaluate(test_data))\n",
    "print(tt.tag(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9094781682641108\n",
      "[('The', 'DT'), ('brown', 'NN'), ('fox', 'NN'), ('is', 'VBZ'), ('quick', 'JJ'), ('and', 'CC'), ('he', 'PRP'), ('is', 'VBZ'), ('jumping', 'VBG'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'NN'), ('dog', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "def combined_tagger(train_data, taggers, backoff=None):\n",
    "    for tagger in taggers:\n",
    "        backoff = tagger(train_data, backoff=backoff)\n",
    "    return backoff\n",
    "\n",
    "\n",
    "ct = combined_tagger(train_data=train_data, \n",
    "                     taggers=[UnigramTagger, BigramTagger, TrigramTagger],\n",
    "                     backoff=rt)\n",
    "\n",
    "print(ct.evaluate(test_data))        \n",
    "print(ct.tag(tokens))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9306806079969019\n",
      "[('The', 'DT'), ('brown', 'JJ'), ('fox', 'NN'), ('is', 'VBZ'), ('quick', 'JJ'), ('and', 'CC'), ('he', 'PRP'), ('is', 'VBZ'), ('jumping', 'VBG'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'VBG')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.classify import NaiveBayesClassifier, MaxentClassifier\n",
    "from nltk.tag.sequential import ClassifierBasedPOSTagger\n",
    "\n",
    "nbt = ClassifierBasedPOSTagger(train=train_data,\n",
    "                               classifier_builder=NaiveBayesClassifier.train)\n",
    "\n",
    "print(nbt.evaluate(test_data))\n",
    "print(nbt.tag(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -3.82864        0.007\n",
      "             2          -0.76176        0.957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/song-yeongsug/anaconda3/lib/python3.7/site-packages/nltk/classify/maxent.py:1392: RuntimeWarning: overflow encountered in power\n",
      "  exp_nf_delta = 2 ** nf_delta\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Final               nan        0.984\n",
      "0.9270984606447865\n",
      "[('The', 'DT'), ('brown', 'NN'), ('fox', 'NN'), ('is', 'VBZ'), ('quick', 'JJ'), ('and', 'CC'), ('he', 'PRP'), ('is', 'VBZ'), ('jumping', 'VBG'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'NN'), ('dog', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "# try this out for fun!\n",
    "met = ClassifierBasedPOSTagger(train=train_data,\n",
    "                               classifier_builder=MaxentClassifier.train)\n",
    "print(met.evaluate(test_data))                           \n",
    "print(met.tag(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shallow Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommended Shallow Parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The brown fox is quick and he is jumping over the lazy dog\n"
     ]
    }
   ],
   "source": [
    "from pattern.en import parsetree, Chunk\n",
    "from nltk.tree import Tree\n",
    "\n",
    "sentence = 'The brown fox is quick and he is jumping over the lazy dog'\n",
    "\n",
    "tree = parsetree(sentence)\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Chunk('The brown fox/NP'), Chunk('is/VP'), Chunk('quick/ADJP'), Chunk('he/NP'), Chunk('is jumping/VP'), Chunk('over/PP'), Chunk('the lazy dog/NP')]\n",
      "NP -> [('The', 'DT'), ('brown', 'JJ'), ('fox', 'NN')]\n",
      "VP -> [('is', 'VBZ')]\n",
      "ADJP -> [('quick', 'JJ')]\n",
      "NP -> [('he', 'PRP')]\n",
      "VP -> [('is', 'VBZ'), ('jumping', 'VBG')]\n",
      "PP -> [('over', 'IN')]\n",
      "NP -> [('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for sentence_tree in tree:\n",
    "    print(sentence_tree.chunks)\n",
    "    \n",
    "for sentence_tree in tree:\n",
    "    for chunk in sentence_tree.chunks:\n",
    "        print(chunk.type, '->', [(word.string, word.type) \n",
    "                                 for word in chunk.words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sentence_tree(sentence, lemmatize=False):\n",
    "    sentence_tree = parsetree(sentence, \n",
    "                              relations=True, \n",
    "                              lemmata=lemmatize)\n",
    "    return sentence_tree[0]\n",
    "    \n",
    "def get_sentence_tree_constituents(sentence_tree):\n",
    "    return sentence_tree.constituents()\n",
    "    \n",
    "def process_sentence_tree(sentence_tree):\n",
    "    \n",
    "    tree_constituents = get_sentence_tree_constituents(sentence_tree)\n",
    "    processed_tree = [\n",
    "                        (item.type,\n",
    "                         [\n",
    "                             (w.string, w.type)\n",
    "                             for w in item.words\n",
    "                         ]\n",
    "                        )\n",
    "                        if type(item) == Chunk\n",
    "                        else ('-',\n",
    "                              [\n",
    "                                   (item.string, item.type)\n",
    "                              ]\n",
    "                             )\n",
    "                             for item in tree_constituents\n",
    "                    ]\n",
    "    \n",
    "    return processed_tree\n",
    "    \n",
    "def print_sentence_tree(sentence_tree):\n",
    "    \n",
    "\n",
    "    processed_tree = process_sentence_tree(sentence_tree)\n",
    "    processed_tree = [\n",
    "                        Tree( item[0],\n",
    "                             [\n",
    "                                 Tree(x[1], [x[0]])\n",
    "                                 for x in item[1]\n",
    "                             ]\n",
    "                            )\n",
    "                            for item in processed_tree\n",
    "                     ]\n",
    "\n",
    "    tree = Tree('S', processed_tree )\n",
    "    print(tree)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The brown fox is quick and he is jumping over the lazy dog\n",
      "(S\n",
      "  (NP (DT The) (JJ brown) (NN fox))\n",
      "  (VP (VBZ is))\n",
      "  (ADJP (JJ quick))\n",
      "  (- (CC and))\n",
      "  (NP (PRP he))\n",
      "  (VP (VBZ is) (VBG jumping))\n",
      "  (PP (IN over))\n",
      "  (NP (DT the) (JJ lazy) (NN dog)))\n"
     ]
    }
   ],
   "source": [
    "def visualize_sentence_tree(sentence_tree):\n",
    "    \n",
    "    processed_tree = process_sentence_tree(sentence_tree)\n",
    "    processed_tree = [\n",
    "                        Tree( item[0],\n",
    "                             [\n",
    "                                 Tree(x[1], [x[0]])\n",
    "                                 for x in item[1]\n",
    "                             ]\n",
    "                            )\n",
    "                            for item in processed_tree\n",
    "                     ]\n",
    "    tree = Tree('S', processed_tree )\n",
    "    tree.draw()\n",
    "    \n",
    "    \n",
    "t = create_sentence_tree(sentence)\n",
    "print(t)\n",
    "\n",
    "pt = process_sentence_tree(t)\n",
    "pt\n",
    "\n",
    "print_sentence_tree(t)\n",
    "visualize_sentence_tree(t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Your Own Shallow Parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP A/DT Lorillard/NNP spokewoman/NN)\n",
      "  said/VBD\n",
      "  ,/,\n",
      "  ``/``\n",
      "  (NP This/DT)\n",
      "  is/VBZ\n",
      "  (NP an/DT old/JJ story/NN)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import treebank_chunk\n",
    "data = treebank_chunk.chunked_sents()\n",
    "train_data = data[:4000]\n",
    "test_data = data[4000:]\n",
    "\n",
    "# view what a sample data point looks like\n",
    "print(train_data[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 'DT'), ('quick', 'JJ'), ('fox', 'NN'), ('jumped', 'VBD'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "simple_sentence = 'the quick fox jumped over the lazy dog'\n",
    "\n",
    "from nltk.chunk import RegexpParser\n",
    "from pattern.en import tag\n",
    "\n",
    "# get POS tagged sentence\n",
    "tagged_simple_sent = tag(simple_sentence)\n",
    "print(tagged_simple_sent)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP the/DT quick/JJ fox/NN)\n",
      "  jumped/VBD\n",
      "  over/IN\n",
      "  (NP the/DT lazy/JJ dog/NN))\n",
      "(S\n",
      "  (NP the/DT quick/JJ fox/NN)\n",
      "  jumped/VBD\n",
      "  over/IN\n",
      "  (NP the/DT lazy/JJ dog/NN))\n"
     ]
    }
   ],
   "source": [
    "# illustrate NP chunking based on explicit chunk patterns\n",
    "chunk_grammar = \"\"\"\n",
    "NP: {<DT>?<JJ>*<NN.*>}\n",
    "\"\"\"\n",
    "rc = RegexpParser(chunk_grammar)\n",
    "c = rc.parse(tagged_simple_sent)\n",
    "print(c)\n",
    "\n",
    "chink_grammar = \"\"\"\n",
    "NP: {<.*>+} # chunk everything as NP\n",
    "}<VBD|IN>+{\n",
    "\"\"\"\n",
    "rc = RegexpParser(chink_grammar)\n",
    "c = rc.parse(tagged_simple_sent)\n",
    "print(c)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('brown', 'JJ'), ('fox', 'NN'), ('is', 'VBZ'), ('quick', 'JJ'), ('and', 'CC'), ('he', 'PRP'), ('is', 'VBZ'), ('jumping', 'VBG'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "# view NP chunked sentence using chunking\n",
    "tagged_sentence = tag(sentence)\n",
    "print(tagged_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP The/DT brown/JJ fox/NN)\n",
      "  (VP is/VBZ)\n",
      "  (ADJP quick/JJ)\n",
      "  and/CC\n",
      "  he/PRP\n",
      "  (VP is/VBZ jumping/VBG)\n",
      "  (PP over/IN)\n",
      "  (NP the/DT lazy/JJ dog/NN))\n",
      "ChunkParse score:\n",
      "    IOB Accuracy:  54.5%%\n",
      "    Precision:     25.0%%\n",
      "    Recall:        52.5%%\n",
      "    F-Measure:     33.9%%\n"
     ]
    }
   ],
   "source": [
    "grammar = \"\"\"\n",
    "NP: {<DT>?<JJ>?<NN.*>}  \n",
    "ADJP: {<JJ>}\n",
    "ADVP: {<RB.*>}\n",
    "PP: {<IN>}      \n",
    "VP: {<MD>?<VB.*>+}\n",
    "\n",
    "\"\"\"\n",
    "rc = RegexpParser(grammar)\n",
    "c = rc.parse(tagged_sentence)\n",
    "\n",
    "print(c)\n",
    "\n",
    "print(rc.evaluate(test_data))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP A/DT Lorillard/NNP spokewoman/NN)\n",
      "  said/VBD\n",
      "  ,/,\n",
      "  ``/``\n",
      "  (NP This/DT)\n",
      "  is/VBZ\n",
      "  (NP an/DT old/JJ story/NN)\n",
      "  ./.)\n",
      "(S\n",
      "  (NP A/DT Lorillard/NNP spokewoman/NN)\n",
      "  said/VBD\n",
      "  ,/,\n",
      "  ``/``\n",
      "  (NP This/DT)\n",
      "  is/VBZ\n",
      "  (NP an/DT old/JJ story/NN)\n",
      "  ./.)\n",
      "ChunkParse score:\n",
      "    IOB Accuracy:  99.6%%\n",
      "    Precision:     98.4%%\n",
      "    Recall:       100.0%%\n",
      "    F-Measure:     99.2%%\n",
      "(S\n",
      "  (NP The/DT brown/JJ fox/NN)\n",
      "  is/VBZ\n",
      "  (NP quick/JJ)\n",
      "  and/CC\n",
      "  (NP he/PRP)\n",
      "  is/VBZ\n",
      "  jumping/VBG\n",
      "  over/IN\n",
      "  (NP the/DT lazy/JJ dog/NN))\n",
      "(S\n",
      "  (NP He/PRP)\n",
      "  (VP reckons/VBZ)\n",
      "  (NP the/DT current/JJ account/NN deficit/NN)\n",
      "  (VP will/MD narrow/VB)\n",
      "  (PP to/TO)\n",
      "  (NP only/RB #/# 1.8/CD billion/CD)\n",
      "  (PP in/IN)\n",
      "  (NP September/NNP)\n",
      "  ./.)\n",
      "ChunkParse score:\n",
      "    IOB Accuracy:  89.4%%\n",
      "    Precision:     80.8%%\n",
      "    Recall:        86.0%%\n",
      "    F-Measure:     83.3%%\n"
     ]
    }
   ],
   "source": [
    "from nltk.chunk.util import tree2conlltags, conlltags2tree\n",
    "\n",
    "train_sent = train_data[7]\n",
    "print(train_sent)\n",
    "\n",
    "wtc = tree2conlltags(train_sent)\n",
    "wtc\n",
    "\n",
    "tree = conlltags2tree(wtc)\n",
    "print(tree)\n",
    "    \n",
    "\n",
    "def conll_tag_chunks(chunk_sents):\n",
    "  tagged_sents = [tree2conlltags(tree) for tree in chunk_sents]\n",
    "  return [[(t, c) for (w, t, c) in sent] for sent in tagged_sents]\n",
    "  \n",
    "def combined_tagger(train_data, taggers, backoff=None):\n",
    "    for tagger in taggers:\n",
    "        backoff = tagger(train_data, backoff=backoff)\n",
    "    return backoff\n",
    "  \n",
    "from nltk.tag import UnigramTagger, BigramTagger\n",
    "from nltk.chunk import ChunkParserI\n",
    "\n",
    "class NGramTagChunker(ChunkParserI):\n",
    "    \n",
    "  def __init__(self, train_sentences, \n",
    "               tagger_classes=[UnigramTagger, BigramTagger]):\n",
    "    train_sent_tags = conll_tag_chunks(train_sentences)\n",
    "    self.chunk_tagger = combined_tagger(train_sent_tags, tagger_classes)\n",
    "\n",
    "  def parse(self, tagged_sentence):\n",
    "    if not tagged_sentence: \n",
    "        return None\n",
    "    pos_tags = [tag for word, tag in tagged_sentence]\n",
    "    chunk_pos_tags = self.chunk_tagger.tag(pos_tags)\n",
    "    chunk_tags = [chunk_tag for (pos_tag, chunk_tag) in chunk_pos_tags]\n",
    "    wpc_tags = [(word, pos_tag, chunk_tag) for ((word, pos_tag), chunk_tag)\n",
    "                     in zip(tagged_sentence, chunk_tags)]\n",
    "    return conlltags2tree(wpc_tags)\n",
    "\n",
    "\n",
    "ntc = NGramTagChunker(train_data)\n",
    "print(ntc.evaluate(test_data))\n",
    "\n",
    "tree = ntc.parse(tagged_sentence)\n",
    "print(tree)\n",
    "tree.draw()\n",
    "\n",
    "\n",
    "from nltk.corpus import conll2000\n",
    "wsj_data = conll2000.chunked_sents()\n",
    "train_wsj_data = wsj_data[:7500]\n",
    "test_wsj_data = wsj_data[7500:]\n",
    "print(train_wsj_data[10])\n",
    "\n",
    "tc = NGramTagChunker(train_wsj_data)\n",
    "print(tc.evaluate(test_wsj_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependent Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "특징\n",
    "토큰들 사이에 관계에 주목\n",
    "\n",
    "one-to-one mappings "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src= \"de.png\" width = '900'><BR>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#터미널 명령어로 : conda install -c conda-forge spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy.load('en_core_web_sm')\n",
    "from spacy.lang.en import English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')  # <-- an instance of `English` with data loaded in\n",
    "doc = nlp(\"This is a text.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "참고 : https://spacy.io/#getting-started\n",
    "\n",
    "\n",
    "https://www.datacamp.com/community/blog/spacy-cheatsheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependency grammar with 12 productions\n",
      "  'fox' -> 'The'\n",
      "  'fox' -> 'brown'\n",
      "  'quick' -> 'fox'\n",
      "  'quick' -> 'is'\n",
      "  'quick' -> 'and'\n",
      "  'quick' -> 'jumping'\n",
      "  'jumping' -> 'he'\n",
      "  'jumping' -> 'is'\n",
      "  'jumping' -> 'dog'\n",
      "  'dog' -> 'over'\n",
      "  'dog' -> 'the'\n",
      "  'dog' -> 'lazy'\n",
      "(quick (fox The brown) is and (jumping he is (dog over the lazy)))\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "dependency_rules = \"\"\"\n",
    "'fox' -> 'The' | 'brown'\n",
    "'quick' -> 'fox' | 'is' | 'and' | 'jumping'\n",
    "'jumping' -> 'he' | 'is' | 'dog'\n",
    "'dog' -> 'over' | 'the' | 'lazy'\n",
    "\"\"\"\n",
    "\n",
    "dependency_grammar = nltk.grammar.DependencyGrammar.fromstring(dependency_rules)\n",
    "print(dependency_grammar)\n",
    "\n",
    "dp = nltk.ProjectiveDependencyParser(dependency_grammar)\n",
    "res = [item for item in dp.parse(tokens)]\n",
    "tree = res[0] \n",
    "print(tree)\n",
    "\n",
    "tree.draw()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# constituency_parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'parser.png' width = '700'><BR>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP-SBJ (NNP Mr.) (NNP Vinken))\n",
      "  (VP\n",
      "    (VBZ is)\n",
      "    (NP-PRD\n",
      "      (NP (NN chairman))\n",
      "      (PP\n",
      "        (IN of)\n",
      "        (NP\n",
      "          (NP (NNP Elsevier) (NNP N.V.))\n",
      "          (, ,)\n",
      "          (NP (DT the) (NNP Dutch) (VBG publishing) (NN group))))))\n",
      "  (. .))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[NN -> 'shortage',\n",
       " NNP -> 'Silverman',\n",
       " -NONE- -> '*-37',\n",
       " NP -> `` NP , '' NP,\n",
       " NP -> DT `` JJ NN,\n",
       " VBN -> 'calculated',\n",
       " VBN -> 'wanted',\n",
       " PP-DIR -> IN NP PP-TMP,\n",
       " RB -> 'popularly',\n",
       " NP-LOC -> NNP .]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.grammar import Nonterminal\n",
    "from nltk.corpus import treebank\n",
    "\n",
    "training_set = treebank.parsed_sents()\n",
    "\n",
    "print(training_set[1])\n",
    "\n",
    "# extract the productions for all annotated training sentences\n",
    "treebank_productions = list(\n",
    "                        set(production \n",
    "                            for sent in training_set  \n",
    "                            for production in sent.productions()\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "treebank_productions[0:10]\n",
    "  \n",
    "\n",
    "\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('brown', 'JJ'), ('fox', 'NN'), ('is', 'VBZ'), ('quick', 'JJ')]\n"
     ]
    }
   ],
   "source": [
    "sentence = 'The brown fox is quick'\n",
    "\n",
    "# add productions for each word, POS tag\n",
    "for word, tag in treebank.tagged_words():\n",
    "    t = nltk.Tree.fromstring(\"(\"+ tag + \" \" + word  +\")\")\n",
    "    for production in t.productions():\n",
    "        treebank_productions.append(production)\n",
    "\n",
    "# build the PCFG based grammar  \n",
    "treebank_grammar = nltk.grammar.induce_pcfg(Nonterminal('S'), \n",
    "                                         treebank_productions)\n",
    "\n",
    "# build the parser\n",
    "viterbi_parser = nltk.ViterbiParser(treebank_grammar)\n",
    "\n",
    "# get sample sentence tokens\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "# get parse tree for sample sentence\n",
    "result = list(viterbi_parser.parse(tokens))\n",
    "\n",
    "\n",
    "# get tokens and their POS tags\n",
    "from pattern.en import tag as pos_tagger\n",
    "tagged_sent = pos_tagger(sentence)\n",
    "\n",
    "print(tagged_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP-SBJ-77 (DT The) (JJ brown) (NN fox))\n",
      "  (VP (VBZ is) (PRT (JJ quick)))) (p=2.25246e-21)\n"
     ]
    }
   ],
   "source": [
    "sentence = 'The brown fox is quick'\n",
    "\n",
    "# extend productions for sample sentence tokens\n",
    "for word, tag in tagged_sent:\n",
    "    t = nltk.Tree.fromstring(\"(\"+ tag + \" \" + word  +\")\")\n",
    "    for production in t.productions():\n",
    "        treebank_productions.append(production)\n",
    "\n",
    "# rebuild grammar\n",
    "treebank_grammar = nltk.grammar.induce_pcfg(Nonterminal('S'), \n",
    "                                         treebank_productions)                                         \n",
    "\n",
    "# rebuild parser\n",
    "viterbi_parser = nltk.ViterbiParser(treebank_grammar)\n",
    "\n",
    "# get parse tree for sample sentence\n",
    "result = list(viterbi_parser.parse(tokens))\n",
    "\n",
    "print(result[0])\n",
    "result[0].draw()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
